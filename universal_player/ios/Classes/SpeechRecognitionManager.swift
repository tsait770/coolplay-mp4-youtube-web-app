//
// SpeechRecognitionManager.swift
// èªéŸ³ç›£è½ç®¡ç†é¡åˆ¥ï¼Œæ—¨åœ¨è§£æ±º iOS èªéŸ³ç›£è½ 5 ç§’ä¸­æ–·çš„å•é¡Œï¼Œä¸¦å¯¦ç¾æŒçºŒç›£è½æ¨¡å¼ã€‚
// æ¶µè“‹äº†ä»»å‹™æ›¸ä¸­æ‰€æœ‰å¿…è¦çš„ä¿®æ­£é …ç›® (AVAudioSession åˆå§‹åŒ–ã€audioEngine é‡ç½®ã€æŒçºŒç›£è½é‚è¼¯ã€éŒ¯èª¤åµéŒ¯)ã€‚
//

import Foundation
import Speech
import AVFoundation

/// èªéŸ³ç›£è½ç‹€æ…‹
enum SpeechRecognitionState {
    case idle
    case listening
    case stopped(Error?)
    case error(Error)
}

/// èªéŸ³ç›£è½ç®¡ç†é¡åˆ¥
class SpeechRecognitionManager: NSObject {

    // MARK: - Properties

    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "zh-TW")) // å‡è¨­ä½¿ç”¨ç¹é«”ä¸­æ–‡
    private var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    private var recognitionTask: SFSpeechRecognitionTask?
    private let audioEngine = AVAudioEngine()

    // ç‹€æ…‹å›èª¿
    var stateUpdateHandler: ((SpeechRecognitionState) -> Void)?
    // è¾¨è­˜çµæœå›èª¿
    var resultUpdateHandler: ((String, Bool) -> Void)? // (text, isFinal)

    // ä»»å‹™ 4: æŒçºŒç›£è½æ¨¡å¼çš„é—œéµæ¨™èªŒ
    private var isContinuousListening: Bool = false
    private var isRestarting: Bool = false

    // MARK: - Initialization

    override init() {
        super.init()
        speechRecognizer?.delegate = self
    }

    // MARK: - Public Methods

    /// æª¢æŸ¥ä¸¦è«‹æ±‚éº¥å…‹é¢¨å’ŒèªéŸ³è¾¨è­˜æ¬Šé™ (ä»»å‹™ 7)
    func requestPermissions(completion: @escaping (Bool) -> Void) {
        // æª¢æŸ¥ SFSpeechRecognizer æ¬Šé™
        SFSpeechRecognizer.requestAuthorization { authStatus in
            // æª¢æŸ¥ AVAudioSession æ¬Šé™ (éº¥å…‹é¢¨)
            AVAudioSession.sharedInstance().requestRecordPermission { [weak self] granted in
                DispatchQueue.main.async {
                    let speechGranted = authStatus == .authorized
                    let micGranted = granted

                    if !speechGranted || !micGranted {
                        // ä»»å‹™ 7: æ¬Šé™ä¸è¶³ï¼Œæç¤ºç”¨æˆ¶
                        // print("âŒ [ä»»å‹™ 7] æ¬Šé™ä¸è¶³ï¼šèªéŸ³è¾¨è­˜æ¬Šé™: \(authStatus.rawValue), éº¥å…‹é¢¨æ¬Šé™: \(micGranted)")
                        // é€™è£¡å¯ä»¥åŠ å…¥æ›´å‹å¥½çš„ UI æç¤ºï¼Œå¼•å°ç”¨æˆ¶åˆ°è¨­å®šé é¢
                    } else if authStatus == .notDetermined || AVAudioSession.sharedInstance().recordPermission == .undetermined {
                        // é›–ç„¶å·²ç¶“è«‹æ±‚ï¼Œä½†å¦‚æœç‹€æ…‹ä»ç‚ºæœªå®šï¼Œå¯èƒ½éœ€è¦é€²ä¸€æ­¥è™•ç†
                        // print("âš ï¸ [ä»»å‹™ 7] æ¬Šé™ç‹€æ…‹æœªå®šï¼Œè«‹æª¢æŸ¥ Info.plist è¨­å®šã€‚")
                    } else if authStatus == .restricted || authStatus == .denied || AVAudioSession.sharedInstance().recordPermission == .denied {
                        // æ¬Šé™è¢«æ‹’çµ•ï¼Œéœ€è¦æç¤ºç”¨æˆ¶
                        // print("âŒ [ä»»å‹™ 7] æ¬Šé™è¢«æ‹’çµ•ï¼Œè«‹æª¢æŸ¥æ˜¯å¦ç‚º 'å…è¨±ä¸€æ¬¡' æˆ– 'ä¸å…è¨±'ã€‚")
                    }

                    completion(speechGranted && micGranted)
                }
            }
        }
    }

    /// å•Ÿå‹•èªéŸ³ç›£è½
    func startListening(continuous: Bool = false) {
        guard !audioEngine.isRunning else {
            // print("âš ï¸ èªéŸ³å¼•æ“å·²åœ¨é‹è¡Œä¸­ã€‚")
            return
        }

        isContinuousListening = continuous
        isRestarting = false
        stateUpdateHandler?(.listening)

        // ä»»å‹™ 3: ç¢ºä¿èˆŠçš„ä»»å‹™è¢«å–æ¶ˆ
        recognitionTask?.cancel()
        recognitionTask = nil

        // ä»»å‹™ 2: è£œä¸Š audioEngine åˆå§‹åŒ–é‚è¼¯
        audioEngine.stop()
        audioEngine.reset()

        do {
            // ä»»å‹™ 1: è£œä¸Šæ­£ç¢º AVAudioSession åˆå§‹åŒ– (é«˜å„ªå…ˆç´š)
            let audioSession = AVAudioSession.sharedInstance()
            // ä½¿ç”¨ .record é¡åˆ¥ï¼Œ.measurement æ¨¡å¼ï¼Œ.duckOthers é¸é …
            try audioSession.setCategory(.record, mode: .measurement, options: .duckOthers)
            // .notifyOthersOnDeactivation ç¢ºä¿åœ¨åœæ­¢æ™‚é€šçŸ¥å…¶ä»– App
            try audioSession.setActive(true, options: .notifyOthersOnDeactivation)
            
            // æª¢æŸ¥éº¥å…‹é¢¨æ˜¯å¦å¯ç”¨
            guard audioSession.inputIsAvailable else {
                throw NSError(domain: "SpeechRecognitionManager", code: 1001, userInfo: [NSLocalizedDescriptionKey: "éº¥å…‹é¢¨è¼¸å…¥ä¸å¯ç”¨ã€‚"])
            }

            // æª¢æŸ¥è¾¨è­˜å™¨æ˜¯å¦å¯ç”¨
            guard let speechRecognizer = speechRecognizer, speechRecognizer.isAvailable else {
                // ä»»å‹™ 6: æª¢æŸ¥ç¶²è·¯é€£ç·š (é–“æ¥æª¢æŸ¥)
                throw NSError(domain: "SpeechRecognitionManager", code: 1002, userInfo: [NSLocalizedDescriptionKey: "èªéŸ³è¾¨è­˜å™¨ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·š (api.speech.apple.com)ã€‚"])
            }

            recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
            guard let recognitionRequest = recognitionRequest else {
                fatalError("ç„¡æ³•å‰µå»º SFSpeechAudioBufferRecognitionRequest å¯¦ä¾‹")
            }
            
            // è¨­ç½® recognitionRequest å±¬æ€§
            recognitionRequest.shouldReportPartialResults = true // ä»»å‹™ 4: å•Ÿç”¨éƒ¨åˆ†çµæœå›èª¿
            
            // è¨­ç½®éŸ³é »è¼¸å…¥
            let inputNode = audioEngine.inputNode
            let recordingFormat = inputNode.outputFormat(forBus: 0)
            inputNode.installTap(onBus: 0, bufferSize: 1024, format: recordingFormat) { (buffer, when) in
                self.recognitionRequest?.append(buffer)
            }

            // å•Ÿå‹•éŸ³é »å¼•æ“
            audioEngine.prepare()
            try audioEngine.start()

            // å•Ÿå‹•è¾¨è­˜ä»»å‹™
            recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest) { [weak self] (result, error) in
                guard let self = self else { return }

                var isFinal = false
                if let result = result {
                    // ä»»å‹™ 4: è™•ç† partial results
                    self.resultUpdateHandler?(result.bestTranscription.formattedString, result.isFinal)
                    isFinal = result.isFinal
                }

                if error != nil || isFinal {
                    // ä»»å‹™ 5: ç´€éŒ„ recognitionTask.error
                    if let error = error {
                        // print("âŒ [ä»»å‹™ 5] recognitionTask éŒ¯èª¤: \(error.localizedDescription)")
                    }
                    
                    // åœæ­¢ç•¶å‰ session
                    self.stopListeningSession()

                    if self.isContinuousListening && !self.isRestarting {
                        // ä»»å‹™ 4: æŒçºŒç›£è½æ¨¡å¼ - ä»»å‹™å®Œæˆæˆ–å‡ºéŒ¯å¾Œè‡ªå‹•é‡å•Ÿ
                        // print("ğŸ”„ [ä»»å‹™ 4] æŒçºŒç›£è½æ¨¡å¼ï¼šä»»å‹™çµæŸæˆ–å‡ºéŒ¯ï¼Œæ­£åœ¨è‡ªå‹•é‡å•Ÿ...")
                        self.isRestarting = true
                        // å»¶é²é‡å•Ÿä»¥é¿å…è³‡æºç«¶çˆ­
                        DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
                            self.startListening(continuous: true)
                        }
                    } else {
                        // æœ€çµ‚åœæ­¢
                        self.stateUpdateHandler?(.stopped(error))
                    }
                }
            }
            
            // print("âœ… èªéŸ³ç›£è½å•Ÿå‹•æˆåŠŸã€‚")

        } catch {
            // ä»»å‹™ 5: ç´€éŒ„éŒ¯èª¤
            // print("âŒ [ä»»å‹™ 5] å•Ÿå‹•èªéŸ³ç›£è½æ™‚ç™¼ç”ŸéŒ¯èª¤: \(error.localizedDescription)")
            stopListeningSession()
            stateUpdateHandler?(.error(error))
        }
    }

    /// åœæ­¢èªéŸ³ç›£è½
    func stopListening() {
        isContinuousListening = false // åœæ­¢æŒçºŒç›£è½æ¨¡å¼
        stopListeningSession()
        stateUpdateHandler?(.stopped(nil))
        // print("ğŸ›‘ èªéŸ³ç›£è½å·²æ‰‹å‹•åœæ­¢ã€‚")
    }

    /// åœæ­¢ç•¶å‰éŒ„éŸ³ Session (å…§éƒ¨ä½¿ç”¨)
    private func stopListeningSession() {
        // ä»»å‹™ 3: ç¢ºä¿æ‰€æœ‰çµ„ä»¶åœæ­¢
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        recognitionRequest?.endAudio()
        recognitionRequest = nil
        recognitionTask?.cancel() // ç¢ºä¿å–æ¶ˆï¼Œé¿å…æ„å¤–å›èª¿
        recognitionTask = nil

        // åœæ­¢ AVAudioSession
        do {
            try AVAudioSession.sharedInstance().setActive(false)
        } catch {
            // ä»»å‹™ 5: ç´€éŒ„ audioSession ä¸­æ–·é€šçŸ¥ (deactivation error)
            // print("âŒ [ä»»å‹™ 5] åœæ­¢ AVAudioSession éŒ¯èª¤: \(error.localizedDescription)")
        }
    }
}

// MARK: - SFSpeechRecognizerDelegate

extension SpeechRecognitionManager: SFSpeechRecognizerDelegate {
    
    // ä»»å‹™ 6: æª¢æŸ¥è¾¨è­˜å™¨å¯ç”¨æ€§ (é–“æ¥æª¢æŸ¥ç¶²è·¯é€£ç·š)
    func speechRecognizer(_ speechRecognizer: SFSpeechRecognizer, availabilityDidChange available: Bool) {
        if available {
            // print("âœ… [ä»»å‹™ 6] èªéŸ³è¾¨è­˜å™¨å¯ç”¨ã€‚")
        } else {
            // print("âŒ [ä»»å‹™ 6] èªéŸ³è¾¨è­˜å™¨ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥ç¶²è·¯é€£ç·š (api.speech.apple.com)ã€‚")
            // é€™è£¡å¯ä»¥åŠ å…¥è‡ªå‹•åœæ­¢æˆ–æç¤ºç”¨æˆ¶çš„é‚è¼¯
        }
    }
}

// MARK: - AVAudioSession Interruption Handling

extension SpeechRecognitionManager {
    
    /// è¨»å†Š AVAudioSession ä¸­æ–·é€šçŸ¥ (ä»»å‹™ 5)
    func registerForAudioSessionNotifications() {
        NotificationCenter.default.addObserver(self,
                                               selector: #selector(handleInterruption),
                                               name: AVAudioSession.interruptionNotification,
                                               object: nil)
    }
    
    /// è™•ç† AVAudioSession ä¸­æ–· (ä»»å‹™ 5)
    @objc private func handleInterruption(notification: Notification) {
        guard let userInfo = notification.userInfo,
              let typeValue = userInfo[AVAudioSessionInterruptionTypeKey] as? UInt,
              let type = AVAudioSession.InterruptionType(rawValue: typeValue) else {
            return
        }

        switch type {
        case .began:
            // ä»»å‹™ 5: ç´€éŒ„ audioSession ä¸­æ–·é€šçŸ¥ (began)
            // print("âš ï¸ [ä»»å‹™ 5] AVAudioSession ä¸­æ–·é–‹å§‹ (ä¾‹å¦‚ï¼šä¾†é›»)ã€‚")
            // ä¸­æ–·é–‹å§‹æ™‚ï¼Œåœæ­¢ç•¶å‰ç›£è½
            stopListeningSession()
            stateUpdateHandler?(.stopped(nil))
            
        case .ended:
            guard let optionsValue = userInfo[AVAudioSessionInterruptionOptionKey] as? UInt else { return }
            let options = AVAudioSession.InterruptionOptions(rawValue: optionsValue)
            
            // ä»»å‹™ 5: ç´€éŒ„ audioSession ä¸­æ–·é€šçŸ¥ (ended)
            // print("âœ… [ä»»å‹™ 5] AVAudioSession ä¸­æ–·çµæŸã€‚")

            if options.contains(.shouldResume) {
                // å˜—è©¦é‡æ–°å•Ÿå‹•ç›£è½ (å¦‚æœä¹‹å‰æ˜¯æŒçºŒç›£è½æ¨¡å¼)
                // print("ğŸ”„ å˜—è©¦æ¢å¾©ç›£è½...")
                // é€™è£¡éœ€è¦åˆ¤æ–·æ˜¯å¦éœ€è¦è‡ªå‹•æ¢å¾©ï¼Œå¦‚æœç”¨æˆ¶æ‰‹å‹•åœæ­¢å‰‡ä¸æ¢å¾©
                // ç‚ºäº†ç°¡æ½”ï¼Œé€™è£¡ä¸è‡ªå‹•æ¢å¾©ï¼Œè®“ç”¨æˆ¶æ‰‹å‹•é»æ“Š
            }
            
        @unknown default:
            break
        }
    }
}
